{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed, uncomment:\n",
    "# !pip install -U pip tensorflow==2.15.0 tensorflow-addons==0.22.0 matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19afbb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n",
      "[]\n",
      "2.15.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, json, math, random, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c450ae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be saved to: diabetes_mbv3_artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== CONFIG ====\n",
    "DATA_DIR = \"dataset\"                   # Your actual dataset directory\n",
    "IMG_SIZE = 224                         # MobileNetV3 default sizes: 224 works well\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "MODEL_VARIANT = \"small\"                # \"small\" or \"large\"\n",
    "\n",
    "# Export/Artifacts\n",
    "OUT_DIR = \"diabetes_mbv3_artifacts\"    # Local output directory\n",
    "MODEL_NAME = \"mbv3_diabetes_2class\"\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Artifacts will be saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c454203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== AUGMENTATION ====\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "], name=\"augmentation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e19924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Model: \"mbv3_small_diabetes_2class\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " augmentation (Sequential)   (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " gap (GlobalAveragePooling2  (None, 576)               0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "Model: \"mbv3_small_diabetes_2class\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " augmentation (Sequential)   (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " MobilenetV3small (Function  (None, 7, 7, 576)         939120    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " gap (GlobalAveragePooling2  (None, 576)               0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 576)               0         \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1154      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 940274 (3.59 MB)\n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 1154      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 940274 (3.59 MB)\n",
      "Trainable params: 1154 (4.51 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n",
      "Trainable params: 1154 (4.51 KB)\n",
      "Non-trainable params: 939120 (3.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== MODEL ====\n",
    "if MODEL_VARIANT.lower() == \"large\":\n",
    "    base = keras.applications.MobileNetV3Large(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        include_preprocessing=False,\n",
    "        pooling=None\n",
    "    )\n",
    "else:\n",
    "    base = keras.applications.MobileNetV3Small(\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        include_preprocessing=False,\n",
    "        pooling=None\n",
    "    )\n",
    "\n",
    "base.trainable = False  # start with transfer learning (frozen)\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\n",
    "x = data_augmentation(inputs)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D(name=\"gap\")(x)\n",
    "x = layers.Dropout(0.2, name=\"dropout\")(x)\n",
    "\n",
    "# Keep 2-output structure (batch, 2) â€“ softmax gives probabilities\n",
    "outputs = layers.Dense(2, activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=f\"mbv3_{MODEL_VARIANT}_diabetes_2class\")\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2241c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== CALLBACKS ====\n",
    "ckpt_path = str(Path(OUT_DIR) / f\"{MODEL_NAME}_best.keras\")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        ckpt_path, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=False\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=5, restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.2, patience=3, verbose=1\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "790b0a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\tawhi\\miniconda3\\envs\\tf_gpu_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "79/79 [==============================] - 17s 173ms/step - loss: 0.6025 - accuracy: 0.6700 - val_loss: 0.5534 - val_accuracy: 0.6650 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 17s 173ms/step - loss: 0.6025 - accuracy: 0.6700 - val_loss: 0.5534 - val_accuracy: 0.6650 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "79/79 [==============================] - 13s 162ms/step - loss: 0.4672 - accuracy: 0.7880 - val_loss: 0.4753 - val_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 13s 162ms/step - loss: 0.4672 - accuracy: 0.7880 - val_loss: 0.4753 - val_accuracy: 0.7700 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.4304 - accuracy: 0.8092 - val_loss: 0.4519 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 13s 167ms/step - loss: 0.4304 - accuracy: 0.8092 - val_loss: 0.4519 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "79/79 [==============================] - 12s 155ms/step - loss: 0.4151 - accuracy: 0.8160 - val_loss: 0.4435 - val_accuracy: 0.7650 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 12s 155ms/step - loss: 0.4151 - accuracy: 0.8160 - val_loss: 0.4435 - val_accuracy: 0.7650 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "79/79 [==============================] - 12s 156ms/step - loss: 0.3847 - accuracy: 0.8496 - val_loss: 0.4153 - val_accuracy: 0.8467 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 12s 156ms/step - loss: 0.3847 - accuracy: 0.8496 - val_loss: 0.4153 - val_accuracy: 0.8467 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "79/79 [==============================] - 12s 149ms/step - loss: 0.3679 - accuracy: 0.8492 - val_loss: 0.4027 - val_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 12s 149ms/step - loss: 0.3679 - accuracy: 0.8492 - val_loss: 0.4027 - val_accuracy: 0.8200 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.3577 - accuracy: 0.8588 - val_loss: 0.3864 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.3577 - accuracy: 0.8588 - val_loss: 0.3864 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "79/79 [==============================] - 12s 149ms/step - loss: 0.3456 - accuracy: 0.8596 - val_loss: 0.3818 - val_accuracy: 0.8633 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 12s 149ms/step - loss: 0.3456 - accuracy: 0.8596 - val_loss: 0.3818 - val_accuracy: 0.8633 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.3446 - accuracy: 0.8580 - val_loss: 0.3749 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "79/79 [==============================] - 13s 164ms/step - loss: 0.3446 - accuracy: 0.8580 - val_loss: 0.3749 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "Epoch 10/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.3391 - accuracy: 0.8680 - val_loss: 0.3813 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 15s 185ms/step - loss: 0.3391 - accuracy: 0.8680 - val_loss: 0.3813 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.3228 - accuracy: 0.8716 - val_loss: 0.3586 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 15s 186ms/step - loss: 0.3228 - accuracy: 0.8716 - val_loss: 0.3586 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.3217 - accuracy: 0.8724 - val_loss: 0.3572 - val_accuracy: 0.8683 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.3217 - accuracy: 0.8724 - val_loss: 0.3572 - val_accuracy: 0.8683 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "79/79 [==============================] - 13s 158ms/step - loss: 0.3111 - accuracy: 0.8748 - val_loss: 0.3681 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 13s 158ms/step - loss: 0.3111 - accuracy: 0.8748 - val_loss: 0.3681 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.3153 - accuracy: 0.8672 - val_loss: 0.3581 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 13s 168ms/step - loss: 0.3153 - accuracy: 0.8672 - val_loss: 0.3581 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "79/79 [==============================] - 13s 161ms/step - loss: 0.3214 - accuracy: 0.8748 - val_loss: 0.3472 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 13s 161ms/step - loss: 0.3214 - accuracy: 0.8748 - val_loss: 0.3472 - val_accuracy: 0.8667 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.3015 - accuracy: 0.8764 - val_loss: 0.3455 - val_accuracy: 0.8583 - lr: 0.0010\n",
      "79/79 [==============================] - 13s 165ms/step - loss: 0.3015 - accuracy: 0.8764 - val_loss: 0.3455 - val_accuracy: 0.8583 - lr: 0.0010\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 48s 416ms/step - loss: 0.2880 - accuracy: 0.8908 - val_loss: 0.3236 - val_accuracy: 0.8633 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 48s 416ms/step - loss: 0.2880 - accuracy: 0.8908 - val_loss: 0.3236 - val_accuracy: 0.8633 - lr: 1.0000e-05\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 31s 385ms/step - loss: 0.2665 - accuracy: 0.8956 - val_loss: 0.2934 - val_accuracy: 0.8683 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 31s 385ms/step - loss: 0.2665 - accuracy: 0.8956 - val_loss: 0.2934 - val_accuracy: 0.8683 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 31s 393ms/step - loss: 0.2497 - accuracy: 0.8984 - val_loss: 0.2887 - val_accuracy: 0.8733 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 31s 393ms/step - loss: 0.2497 - accuracy: 0.8984 - val_loss: 0.2887 - val_accuracy: 0.8733 - lr: 1.0000e-05\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 31s 388ms/step - loss: 0.2233 - accuracy: 0.9152 - val_loss: 0.2582 - val_accuracy: 0.8800 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 31s 388ms/step - loss: 0.2233 - accuracy: 0.9152 - val_loss: 0.2582 - val_accuracy: 0.8800 - lr: 1.0000e-05\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 31s 390ms/step - loss: 0.2136 - accuracy: 0.9120 - val_loss: 0.2556 - val_accuracy: 0.8817 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 31s 390ms/step - loss: 0.2136 - accuracy: 0.9120 - val_loss: 0.2556 - val_accuracy: 0.8817 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 31s 385ms/step - loss: 0.1956 - accuracy: 0.9244 - val_loss: 0.2482 - val_accuracy: 0.8867 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 31s 385ms/step - loss: 0.1956 - accuracy: 0.9244 - val_loss: 0.2482 - val_accuracy: 0.8867 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 32s 398ms/step - loss: 0.1836 - accuracy: 0.9260 - val_loss: 0.2410 - val_accuracy: 0.8883 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 32s 398ms/step - loss: 0.1836 - accuracy: 0.9260 - val_loss: 0.2410 - val_accuracy: 0.8883 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 31s 394ms/step - loss: 0.1645 - accuracy: 0.9380 - val_loss: 0.2300 - val_accuracy: 0.9050 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 31s 394ms/step - loss: 0.1645 - accuracy: 0.9380 - val_loss: 0.2300 - val_accuracy: 0.9050 - lr: 1.0000e-05\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 30s 381ms/step - loss: 0.1521 - accuracy: 0.9436 - val_loss: 0.2245 - val_accuracy: 0.9017 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 30s 381ms/step - loss: 0.1521 - accuracy: 0.9436 - val_loss: 0.2245 - val_accuracy: 0.9017 - lr: 1.0000e-05\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 31s 392ms/step - loss: 0.1380 - accuracy: 0.9468 - val_loss: 0.2196 - val_accuracy: 0.9083 - lr: 1.0000e-05\n",
      "79/79 [==============================] - 31s 392ms/step - loss: 0.1380 - accuracy: 0.9468 - val_loss: 0.2196 - val_accuracy: 0.9083 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== TRAIN ====\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Unfreeze base for fine-tuning (optional)\n",
    "base.trainable = True\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max(5, EPOCHS//2),\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38ad8560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VAL REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    diabetes       0.92      0.89      0.91       300\n",
      " nondiabetes       0.89      0.93      0.91       300\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.91      0.91      0.91       600\n",
      "weighted avg       0.91      0.91      0.91       600\n",
      "\n",
      "Confusion matrix:\n",
      "[[267  33]\n",
      " [ 22 278]]\n",
      "=== TEST REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    diabetes       1.00      0.80      0.89        25\n",
      " nondiabetes       0.83      1.00      0.91        25\n",
      "\n",
      "    accuracy                           0.90        50\n",
      "   macro avg       0.92      0.90      0.90        50\n",
      "weighted avg       0.92      0.90      0.90        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  5]\n",
      " [ 0 25]]\n",
      "=== TEST REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    diabetes       1.00      0.80      0.89        25\n",
      " nondiabetes       0.83      1.00      0.91        25\n",
      "\n",
      "    accuracy                           0.90        50\n",
      "   macro avg       0.92      0.90      0.90        50\n",
      "weighted avg       0.92      0.90      0.90        50\n",
      "\n",
      "Confusion matrix:\n",
      "[[20  5]\n",
      " [ 0 25]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== EVALUATE ====\n",
    "def eval_and_report(ds, split_name=\"val\"):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch, labels in ds.as_numpy_iterator():\n",
    "        preds = model.predict(batch, verbose=0)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
    "    print(f\"=== {split_name.upper()} REPORT ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "eval_and_report(val_ds, \"val\")\n",
    "if test_ds is not None:\n",
    "    eval_and_report(test_ds, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacc9ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_SavedModel\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_SavedModel\n",
      "Labels saved to: diabetes_mbv3_artifacts\\labels.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== SAVE SAVEDMODEL & LABELS ====\n",
    "saved_model_dir = str(Path(OUT_DIR) / f\"{MODEL_NAME}_SavedModel\")\n",
    "keras.models.save_model(model, saved_model_dir, include_optimizer=False)\n",
    "print(\"SavedModel:\", saved_model_dir)\n",
    "\n",
    "labels_path = Path(OUT_DIR) / \"labels.txt\"\n",
    "with open(labels_path, \"w\") as f:\n",
    "    for name in class_names:\n",
    "        f.write(name + \"\\n\")\n",
    "print(\"Labels saved to:\", labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5359fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite FP32: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_fp32.tflite\n",
      "TFLite FP16: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_fp16.tflite\n",
      "TFLite FP16: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_fp16.tflite\n",
      "TFLite INT8: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_int8.tflite\n",
      "TFLite INT8: diabetes_mbv3_artifacts\\mbv3_diabetes_2class_int8.tflite\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== TFLITE CONVERSION ====\n",
    "\n",
    "def make_representative_dataset(ds, num_batches=50):\n",
    "    def rep():\n",
    "        count = 0\n",
    "        for batch, _ in ds.take(num_batches):\n",
    "            batch = tf.cast(batch, tf.float32) / 255.0\n",
    "            for img in batch:\n",
    "                img = tf.expand_dims(img, 0)\n",
    "                yield [img]\n",
    "                count += 1\n",
    "                if count >= num_batches:\n",
    "                    return\n",
    "    return rep\n",
    "\n",
    "# FP32\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = str(Path(OUT_DIR) / f\"{MODEL_NAME}_fp32.tflite\")\n",
    "with open(tflite_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "print(\"TFLite FP32:\", tflite_path)\n",
    "\n",
    "# FP16 (float16 weight quant)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16 = converter.convert()\n",
    "tflite_fp16_path = str(Path(OUT_DIR) / f\"{MODEL_NAME}_fp16.tflite\")\n",
    "with open(tflite_fp16_path, \"wb\") as f:\n",
    "    f.write(tflite_fp16)\n",
    "print(\"TFLite FP16:\", tflite_fp16_path)\n",
    "\n",
    "# INT8 (full integer quant) â€“ needs representative dataset\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = make_representative_dataset(\n",
    "        train_ds.unbatch().batch(1), num_batches=200\n",
    "    )\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    tflite_int8 = converter.convert()\n",
    "    tflite_int8_path = str(Path(OUT_DIR) / f\"{MODEL_NAME}_int8.tflite\")\n",
    "    with open(tflite_int8_path, \"wb\") as f:\n",
    "        f.write(tflite_int8)\n",
    "    print(\"TFLite INT8:\", tflite_int8_path)\n",
    "except Exception as e:\n",
    "    print(\"INT8 conversion skipped due to error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c468459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== ANDROID/EDGE INFERENCE PARITY (TFLite Python Example) ====\n",
    "# Example on a single image path (replace with your image to test output structure)\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image_for_tflite(img_path, img_size=IMG_SIZE):\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize((img_size, img_size))\n",
    "    x = np.array(img).astype(\"float32\") / 255.0\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "def run_tflite_inference(tflite_file, input_array):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_file)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # Assume single input, single output\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], input_array)\n",
    "    interpreter.invoke()\n",
    "    y = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "    return y\n",
    "\n",
    "# Example usage (uncomment and set an image path):\n",
    "# test_image_path = \"/path/to/sample.jpg\"\n",
    "# x = preprocess_image_for_tflite(test_image_path)\n",
    "# y = run_tflite_inference(tflite_fp16_path, x)\n",
    "# print(\"TFLite output shape:\", y.shape)  # Expect (1, 2)\n",
    "# print(\"TFLite raw output:\", y)\n",
    "# print(\"Predicted class:\", class_names[int(np.argmax(y, axis=1)[0])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f453dd",
   "metadata": {},
   "source": [
    "\n",
    "## Android (Kotlin) â€“ Minimal TFLite Inference\n",
    "Copy `*.tflite` and `labels.txt` to your app's assets. Then:\n",
    "\n",
    "```kotlin\n",
    "import org.tensorflow.lite.Interpreter\n",
    "import android.content.res.AssetFileDescriptor\n",
    "import java.nio.MappedByteBuffer\n",
    "import java.nio.channels.FileChannel\n",
    "\n",
    "fun loadModelFile(assetManager: android.content.res.AssetManager, modelPath: String): MappedByteBuffer {\n",
    "    val fileDescriptor: AssetFileDescriptor = assetManager.openFd(modelPath)\n",
    "    val inputStream = FileInputStream(fileDescriptor.fileDescriptor)\n",
    "    val fileChannel = inputStream.channel\n",
    "    val startOffset = fileDescriptor.startOffset\n",
    "    val declaredLength = fileDescriptor.declaredLength\n",
    "    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)\n",
    "}\n",
    "\n",
    "// Assuming input is 224x224x3 float32 normalized to [0,1]\n",
    "val interpreter = Interpreter(loadModelFile(assets, \"mbv3_diabetes_2class_fp16.tflite\"))\n",
    "val inputShape = interpreter.getInputTensor(0).shape() // [1, 224, 224, 3]\n",
    "val outputShape = interpreter.getOutputTensor(0).shape() // [1, 2]\n",
    "\n",
    "val inputBuffer = ByteBuffer.allocateDirect(1 * 224 * 224 * 3 * 4).order(ByteOrder.nativeOrder())\n",
    "// TODO: Fill inputBuffer with your preprocessed image data [0..1]\n",
    "\n",
    "val outputBuffer = ByteBuffer.allocateDirect(1 * 2 * 4).order(ByteOrder.nativeOrder())\n",
    "interpreter.run(inputBuffer, outputBuffer)\n",
    "\n",
    "outputBuffer.rewind()\n",
    "val probs = FloatArray(2)\n",
    "outputBuffer.asFloatBuffer().get(probs)\n",
    "val classes = assets.open(\"labels.txt\").bufferedReader().readLines()\n",
    "val predIdx = probs.indices.maxBy { probs[it] } ?: 0\n",
    "val predLabel = classes[predIdx]\n",
    "```\n",
    "\n",
    "> The `.tflite` model outputs a 2-length vector per image (same output structure as training). Index-to-label is given by `labels.txt`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
